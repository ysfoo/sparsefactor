# Generated by using Rcpp::compileAttributes() -> do not edit by hand
# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393

#' Variational inference for the sparse factor model
#'
#' Runs coordinate ascent variational inference (CAVI) assuming a mean-field approximation.
#'
#' The mean-field approximation is almost fully factorised, except that the dependency between corresponding entries of \strong{L} and \strong{Z} cannot be removed.
#'
#' @param ymat Data matrix, rows corresponding to features and columns corresponding to samples. May contain \code{NA}s.
#' @param pivec Vector of sparsity hyperparameters for each factor.
#' @param ptaushape Shape hyperparameter of the gamma prior for the feature-specific precision of the noise.
#' @param ptaurate Rate hyperparameter of the gamma prior for the feature-specific precision of the noise.
#' @param palphashape Shape hyperparameter of the gamma prior for the factor-specific precision of the loading factors.
#' @param palpharate Rate hyperparameter of the gamma prior for simulating the factor-specific precision of the loading factors.
#' @param check Convergence is checked once every \code{check} iterations. This is done because the ELBO is rather expensive to calculate.
#' @param save Variational parameters are stored (to be returned) once every \code{save} iterations. This is done to save memory. If \code{save} is \code{0}, only the final set of variational parameters is returned.
#' @param max_iter Maximum number of iterations to run.
#' @param tol_elbo Optimisation terminates when the difference in ELBO is less than \code{tol_elbo}.
#' @param tol_z Additional option for optimisation to terminate when the maximum difference in the variational parameter for \strong{Z} is less than \code{tol_z}.
#' @param seed Random seed. No seed is set when \code{seed} is \code{-1}.
#'
#' @return A list of variational parameters for each iteration saved.
#' \describe{
#' \item{lmean}{Mean of the normal variational approximation for the nonzero loading factors.}
#' \item{lsig}{Variance of the normal variational approximation for the nonzero loading factors.}
#' \item{fmean}{Mean of the normal variational approximation for the activation weights.}
#' \item{fsig}{Variance of the normal variational approximation for the activation weights.}
#' \item{zmean}{Mean of the Bernoulli variational approximation for the connectivity structure.}
#' \item{taushape}{Shape parameter of the gamma variational approximation for the feature-specific precision of the noise.}
#' \item{taurate}{Rate parameter of the gamma variational approximation for the feature-specific precision of the noise.}
#' \item{alphashape}{Shape parameter of the gamma variational approximation for the factor-specific precision of the loading factors.}
#' \item{alpharate}{Rate parameter of the gamma variational approximation for the factor-specific precision of the loading factors.}
#' \item{elbo}{Evidence lower bound.}
#' \item{iter}{Iteration number.}
#' \item{time}{Time when this iteration finished (in seconds).}
#' }
#'
#' @export
cavi <- function(ymat, pivec, ptaushape, ptaurate, palphashape, palpharate, check = 100L, save = 0L, max_iter = 5000L, tol_elbo = 1e-10, tol_z = 0, seed = -1L) {
    .Call(`_sparsefactor_cavi`, ymat, pivec, ptaushape, ptaurate, palphashape, palpharate, check, save, max_iter, tol_elbo, tol_z, seed)
}

#' Variational inference for the sparse factor model with factor dependency for activation weights
#'
#' Runs coordinate ascent variational inference (CAVI) assuming a mean-field approximation, where factor dependency is modelled for the activation weights.
#'
#' Same functionality as \code{\link{cavi}}, except each column of the activation weights has a variational approximation, instead of each entry. These variational approximations are multivariate normal distributions, and share the same covariance matrix across columns. Dependency between factors is modelled, however, \code{ymat} must have no \code{NA}s.
#'
#' @export
cavi_fdep <- function(ymat, pivec, ptaushape, ptaurate, palphashape, palpharate, check = 100L, save = 0L, max_iter = 5000L, tol_elbo = 1e-10, tol_z = 0, seed = -1L) {
    .Call(`_sparsefactor_cavi_fdep`, ymat, pivec, ptaushape, ptaurate, palphashape, palpharate, check, save, max_iter, tol_elbo, tol_z, seed)
}

#' Variational inference for the sparse factor model with no missing data
#'
#' Runs coordinate ascent variational inference (CAVI) assuming a mean-field approximation.
#'
#' Same functionality as \code{\link{cavi}}, except that \code{ymat} must have no \code{NA}s.
NULL

#' MCMC for the sparse factor model
#'
#' Runs a MCMC using a collapsed Gibbs sampler, where \strong{L} is marginalised out of the conditional distribution of \strong{Z}.
#'
#' @param n_samples Number of samples in MCMC chain, excluding burn-in samples.
#' @param ymat Data matrix, rows corresponding to features and columns corresponding to samples. May contain \code{NA}s.
#' @param pivec Vector of sparsity hyperparameters for each factor.
#' @param ptaushape Shape hyperparameter of the gamma prior for the feature-specific precision of the noise.
#' @param ptaurate Rate hyperparameter of the gamma prior for the feature-specific precision of the noise.
#' @param palphashape Shape hyperparameter of the gamma prior for the factor-specific precision of the loading factors.
#' @param palpharate Rate hyperparameter of the gamma prior for simulating the factor-specific precision of the loading factors.
#' @param burn_in Number of burn-in samples (these are discarded).
#' @param thin Discard all but one sample for every \code{thin} samples generated.
#' @param seed Random seed. No seed is set when \code{seed} is \code{-1}.
#'
#' @return A list of \code{n_samples} MCMC samples.
#' \describe{
#' \item{lmat}{\eqn{T}-by-\eqn{G}-by-\eqn{K} array of the sampled loading factors.}
#' \item{fmat}{\eqn{T}-by-\eqn{K}-by-\eqn{N} array of the sampled activation weights.}
#' \item{zmat}{\eqn{T}-by-\eqn{G}-by-\eqn{K} array of the sampled connectivity structures.}
#' \item{tau}{\eqn{T}-by-\eqn{G} matrix of the sampled feature-specific precisions of the noise.}
#' \item{alpha}{\eqn{T}-by-\eqn{K} matrix of the sampled factor-specific precisions of the loading factors.}
#' \item{time}{Vector of sampling times (in seconds) of when samples were generated.}
#' }
#'
#' @export
gibbs <- function(n_samples, ymat, pivec, ptaushape, ptaurate, palphashape, palpharate, burn_in = 0L, thin = 1L, seed = -1L) {
    .Call(`_sparsefactor_gibbs`, n_samples, ymat, pivec, ptaushape, ptaurate, palphashape, palpharate, burn_in, thin, seed)
}

#' MCMC for the sparse factor model with no missing data
#'
#' Runs a MCMC using a collapsed Gibbs sampler, where \strong{L} is marginalised out of the conditional distribution of \strong{Z}.
#'
#' Same functionality as \code{\link{gibbs}}, except that \code{ymat} must have no \code{NA}s.
NULL

#' Relabel factors within MCMC samples
#'
#' Takes a list of samples and relabels the factors such that each entry of F resembles a normal distribution.
#'
#' The negative log-likelihood is iteratively minimised by solving linear assignment problems via the Jonker-Volgenant algorithm. The algorithm is implemented by Tomas Kazmar (https://github.com/gatagat/lap).
#'
#' @param samples A list of \eqn{T} MCMC samples as returned by \code{\link{gibbs}}.
#' \describe{
#' \item{lmat}{\eqn{T}-by-\eqn{G}-by-\eqn{K} array of the sampled loading factors.}
#' \item{fmat}{\eqn{T}-by-\eqn{K}-by-\eqn{N} array of the sampled activation weights.}
#' \item{zmat}{\eqn{T}-by-\eqn{G}-by-\eqn{K} array of the sampled connectivity structures.}
#' \item{tau}{\eqn{T}-by-\eqn{G} matrix of the sampled feature-specific precisions of the noise.}
#' \item{alpha}{\eqn{T}-by-\eqn{K} matrix of the sampled factor-specific precisions of the loading factors.}
#' \item{time}{Vector of sampling times of when samples were generated.}
#' }
#' @param tol Relabelling algorithm terminates when the change in likelihood is smaller than \code{tol}.
#' @param print_action Boolean for whether to print the final means and variances of the target normal distributions.
#' @param print_cost Boolean for whether to print the negative log-likelihood at each iteration.
#' @param to_clone Boolean for whether to copy \code{samples}.
#'
#' @return A modified copy of \code{samples} with relabelled factors.
#'
#' @export
relabel_samples <- function(samples, tol = 1e-8, print_action = FALSE, print_cost = FALSE, to_clone = TRUE) {
    .Call(`_sparsefactor_relabel_samples`, samples, tol, print_action, print_cost, to_clone)
}

#' Relabelling factors for posterior summary to match a target
#'
#' Takes posterior mean and variance of the activation matrix and finds the relabelling needed for it to match a target (e.g. simualted dataset).
#'
#' This is done by minimising a loss function via solving a linear assignment problem via the Jonker-Volgenant algorithm. The algorithm is implemented by Tomas Kazmar (https://github.com/gatagat/lap).
#'
#' @param fmeans Posterior mean of the activation matrix.
#' @param fsigs Posterior variance of the activation matrix.
#' @param fmat Target activation matrix.
#' @param print_mat Boolean for whether to print the cost matrix of the underlying linear assignment problem.
#'
#' @return A list of the permutation and signflips needed for the posterior summary to match the target. The permutation should be applied before the signflips.
#' \item{permutation}{Permutation (of 1 to K) to apply to the factors of the posterior summary.}
#' \item{sign}{Vector of 1s and -1s to multiply to the factors of the posterior summary after applying the permutation.}
#'
#' @export
get_relabelling <- function(fmeans, fsigs, fmat, print_mat = FALSE) {
    .Call(`_sparsefactor_get_relabelling`, fmeans, fsigs, fmat, print_mat)
}

